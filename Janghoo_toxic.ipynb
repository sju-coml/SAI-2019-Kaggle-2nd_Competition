{"cells":[{"metadata":{},"cell_type":"markdown","source":"On 2018, Kaggle launched a competition in association with Jigsaw/Conversation AI to classify toxic comments. The original competition can be viewed [here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). A toxic comment is a comment that is rude, disrespectful or otherwise likely to make someone leave a discussion. The goal of that competition was to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.\n\nHowever, the models developed in that competition unfortunately associated the targetted group with toxicity, i.e. \"gay\". For example, a comment like \"I am a gay woman\" would be classified as toxic. This happened as the examples of identities associated with toxicity outnumbered neutral comments regarding the same identity.\n\nTherefore, the same team launched a new competition to recognize unintended bias towards identities. We are asked to use a dataset labeled with the associated identity. Let's look into the dataset and understand it first, so we can create a model that can better deal with bias.\n\nAt the time I started this notebook, I looked at the kernel https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw to have a quick start. So I want to take a moment to thank the Author for the kernel. The purpose of this Kernel is to walk Kaggle newbies through the process of data exploration and visualization. In this notebook, we will use Pandas to do a little bit of data wrangling and Plotly and Seaborn to visualize the result of our wrangling. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Section 1: Understanding The Shape of Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, lets count how much data we have!\ntrain_len, test_len = len(train_df.index), len(test_df.index)\nprint(f'train size: {train_len}, test size: {test_len}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# also, lets take a quick look at what we have \ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> As you can see from the table above, a large portion of the data is missing the identity tag. However, as the number is the same for the tags, <br> I assume that the data is complete for the part which has identity tags."},{"metadata":{"trusted":true},"cell_type":"code","source":"# its always a good idea to count the amount of missing values before diving into any analysis\n# Lets also see how many missing values (in percentage) we are dealing with\nmiss_val_train_df = train_df.isnull().sum(axis=0) / train_len\nmiss_val_train_df = miss_val_train_df[miss_val_train_df > 0] * 100\nmiss_val_train_df\n# 비어있는 value 의 percent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Section 2: Identity Analysis and Barplots\n\nNow that we know a large portion of our dataset is doesn't have the group identity, (for now) we can drop it before we do any basic analysis. First simple and interesting question to answer would be which identity appears the most in the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets create a list of all the identities tagged in this dataset. This list given in the data section of this competition. \nidentities = ['male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian',\n              'bisexual','other_sexual_orientation','christian','jewish','muslim','hindu','buddhist',\n              'atheist','other_religion','black','white','asian','latino','other_race_or_ethnicity',\n              'physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness',\n              'other_disability']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the dataframe with identities tagged\n# 해석 : 댓글 내용에 대한 속성들로 이루어진 데이터프레임을 따로 만들어 보자\n\ntrain_labeled_df = train_df.loc[:, ['target'] + identities ].dropna()\n# 문법 설명 :  loc[행, 열]. 위 문장을 보면 모든 행을 다 포함 (:) 그리고 열은 ['target'] 과 identities 리스트에 있는 내용을 포함함.\n\n\n# lets define toxicity as a comment with a score being equal or .5\n# in that case we divide it into two dataframe so we can count toxic vs non toxic comment per identity\n# 해석 : 댓글의 독성을 toxicity > 0.5 로 정의해 보자.\n# 해석 : 그리고, 데이터 프레임을 둘로 나누어 보자. 0.5를 넘는 애들과 0.5가 안 되는 애들로!\n\n\ntoxic_df = train_labeled_df[train_labeled_df['target'] >= .5][identities]\n# 문법 설명 : 데이터프레임에서 [ target 값이 0.5보다 큰 놈들을 골라내고 ], 그 골라내진 데이터 프레임의 column name 이 identities 에 있는 것만 저장한다.\n\nnon_toxic_df = train_labeled_df[train_labeled_df['target'] < .5][identities]\n# 문법 설명 : 데이터프레임에서 [ target 값이 0.5보다 큰 놈들을 골라내고 ], 그 골라내진 데이터 프레임의 column name 이 identities 에 있는 것만 저장한다.\n\ntrain_df.dropna().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labeled_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# at first, we just want to consider the identity tags in binary format. So if the tag is any value other than 0 we consider it as 1.\n# 해석 : 우선, 우리는 단지  0 아니면 1 형태로 된 \"댓글 내용에 대한 속성값\"을 고려해보고 싶어. \n# 해석 : 그러니까, 만약 그 어느 속성값이든 0보다 크면, 우리는 그걸 1이라고 여겨 버린 거지.\n\ntoxic_count = toxic_df.where(train_labeled_df == 0, other = 1).sum()\n# 문법 설명 : toxic_df 에 들어있는 모든 value 들을 보는데, 그 value 값이 0 인 것을 \n\n#non_toxic_count = non_toxic_df.where(train_labeled_df == 0, other = 1).sum()\n\ntoxic_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we can concat the two series together to get a toxic count vs non toxic count for each identity\ntoxic_vs_non_toxic = pd.concat([toxic_count, non_toxic_count], axis=1)\ntoxic_vs_non_toxic = toxic_vs_non_toxic.rename(index=str, columns={1: \"non-toxic\", 0: \"toxic\"})\n# here we plot the stacked graph but we sort it by toxic comments to (perhaps) see something interesting\ntoxic_vs_non_toxic.sort_values(by='toxic').plot(kind='bar', stacked=True, figsize=(30,10), fontsize=20).legend(prop={'size': 20})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The diagram above is certainly one way of looking at our data. However, it is missing the complete picture because we are not using two things. First, for each example we have a score (target) of how toxic the comment is. Second, each identity also has a value between 0 to 1 to identify how much they have been targeted. We can use this two aspects to our advantage and see which identities are more frequently related to toxic comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we multiply each identity with the target\nweighted_toxic = train_labeled_df.iloc[:, 1:].multiply(train_labeled_df.iloc[:, 0], axis=\"index\").sum() \n# changing the value of identity to 1 or 0 only and get comment count per identity group\nidentity_label_count = train_labeled_df[identities].where(train_labeled_df == 0, other = 1).sum()\n# then we divide the target weighted value by the number of time each identity appears\nweighted_toxic = weighted_toxic / identity_label_count\nweighted_toxic = weighted_toxic.sort_values(ascending=False)\n# plot the data using seaborn like before\nplt.figure(figsize=(30,20))\nsns.set(font_scale=3)\nax = sns.barplot(x = weighted_toxic.values , y = weighted_toxic.index, alpha=0.8)\nplt.ylabel('Demographics')\nplt.xlabel('Weighted Toxicity')\nplt.title('Weighted Analysis of Most Frequent Identities')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the graph above, the two race based identities (White and Black) and religion based identities (Muslim and Jews) are heavily associated with toxic comments. "},{"metadata":{},"cell_type":"markdown","source":"## Section 3: Time Series Analysis\nThe dataset also has a 'created_date' field, which tells us when the comment was made. We can use this field to do some time series analysis. In this section, we will also use plotly for simplifying our visualization needs.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets take the dataset with identitiy tags, created date, and target column\nwith_date_df = train_df.loc[:, ['created_date', 'target'] + identities].dropna()\n# next we will create a weighted dataframe for each identity tag (like we did before)\n# first we divide each identity tag with the total value it has in the dataset\nweighted_df = with_date_df.iloc[:, 2:] / with_date_df.iloc[:, 2:].sum()\n# then we multiplty this value with the target \ntarget_weighted_df = weighted_df.multiply(with_date_df.iloc[:, 1], axis=\"index\")\n# lets add a column to count the number of comments\ntarget_weighted_df['comment_count'] = 1\n# now we add the date to our newly created dataframe (also parse the text date as datetime)\ntarget_weighted_df['created_date'] = pd.to_datetime(with_date_df['created_date'].apply(lambda dt: dt[:10]))\n# now we can do a group by of the created date to count the number of times a identity appears for that date\nidentity_weight_per_date_df = target_weighted_df.groupby(['created_date']).sum().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets group most of the identities into three major categories as follows for simplified analysis\nraces = ['black','white','asian','latino','other_race_or_ethnicity']\nreligions = ['atheist', 'buddhist', 'christian', 'hindu', 'muslim', 'jewish','other_religion']\nsexual_orientation = ['heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets create a column to aggregate our weighted toxicity score per identity group\nidentity_weight_per_date_df['races_total'] = identity_weight_per_date_df[races].sum(axis=1)\nidentity_weight_per_date_df['religions_total'] = identity_weight_per_date_df[religions].sum(axis=1)\nidentity_weight_per_date_df['sexual_orientation_total'] = identity_weight_per_date_df[sexual_orientation].sum(axis=1)\n# and then plot a time-series line plot per identity group\nidentity_weight_per_date_df[['races_total', 'religions_total', 'sexual_orientation_total']].plot(figsize=(15,7), linewidth=1, fontsize=15) \nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first sight, what a sad graph to look at. First of all, there are many many spikes in our dataset, so we should zoom into each identity per group to see more. Furthermore, we notice a trend of more toxic messages as time goes by. What a depressing sight :( However, we should keep in mind that the distribution of data over the timeline is probably skewed. Lets check it out.\n\nAlthough there are many spikes in this dataset, there are two that really stands out to me.\n\n1. There huge spike of toxic comments against religion between 2017-01 to 2017-04. This might be because Donald Trump took office in January 2017, and during this time both people were very toxic toward various religions.\n\n2. There is a big spike on toxicity towards races in the middle period between 2017-07 to 2017-10. I did a search on [Goolge Trends on topic that was probabily hot at that time, \"black lives matter\"](https://trends.google.com/trends/explore?date=2017-01-07%202017-12-31&geo=US&q=black%20lives%20matter). Over there we can see that there is a spike around 13th August 2017, which looks like the period we are interested in. Then I found an [aritlce](http://https://theweek.com/10things/713315/10-things-need-know-today-august-13-2017) from \"The Week\" that helped me understand this spike, as it summarizes all the major events from that week. At that time the \"Charlottesville white nationalist rally\", which probably the reason for this spike. \n\nIt isn't my intention to go through each spike and understand what happened there. **I rather wanted to convey the point that toxicity in the comments typically seems to be triggered or an aftermath of certain events.** A little intuative and obvious, but it's always good to point out the obvious with data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_weight_per_date_df['comment_count'].plot(figsize=(15,7), linewidth=1, fontsize=15)\nplt.xlabel('Comment Date', fontsize = 15)\nplt.ylabel('Total Comments', fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we dont have an even distribution of data for our timeline, we can use the comment count to get a better relative weighted toxic score and see where the peaks are."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets divide by the comment count for the date to get a relative weighted toxic score\nidentity_weight_per_date_df['races_rel'] = identity_weight_per_date_df['races_total'] / identity_weight_per_date_df['comment_count']\nidentity_weight_per_date_df['religions_rel'] = identity_weight_per_date_df['religions_total'] / identity_weight_per_date_df['comment_count']\nidentity_weight_per_date_df['sexual_orientation_rel'] = identity_weight_per_date_df['sexual_orientation_total']  / identity_weight_per_date_df['comment_count']\n# now lets plot the data\nidentity_weight_per_date_df[['races_rel', 'religions_rel', 'sexual_orientation_rel']].plot(figsize=(15,7), linewidth=1, fontsize=20) \nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Relative Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot relative weighted toxic score for each identity of races\nidentity_weight_per_date_df[races].div(identity_weight_per_date_df['comment_count'], axis=0).plot(figsize=(15,7), linewidth=1, fontsize=15)\nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Relative Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot relative weighted toxic score for each identity of religions\nidentity_weight_per_date_df[religions].div(identity_weight_per_date_df['comment_count'], axis=0).plot(figsize=(15,7), linewidth=1, fontsize=15)\nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Relative Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot relative weighted toxic score for each identity of sexual orientation\nidentity_weight_per_date_df[sexual_orientation].div(identity_weight_per_date_df['comment_count'], axis=0).plot(figsize=(15,7), linewidth=1, fontsize=20)\nplt.legend(loc=2, prop={'size': 15})\nplt.xlabel('Comment Date', fontsize=15)\nplt.ylabel('Relative Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting! After looking at the visualization above, we can conclude the few following things:\n\n* Amount of hate or toxic comments directed toward any of the group isn't constant\n* The dataset is full of spikes or relative maximums\n* This probably happens due to certain events that triggers hate toward a particular identity"},{"metadata":{},"cell_type":"markdown","source":"## Section 4: Extracting the Peaks / Relative Maximums\nAs we saw in the last section, since there are so many spikes in our dataset, it is very easy to get distracted. To have a better idea we must zoom out. In this section we do that by extracting the peaks. This could be challenging, as we are not just looking for one global maximum. On the other hand, even if we sort it, we will not be able to find all the relative maximums without writing a complex algorithm to do so.\n\nFortunately, there is a great algorithm from **scipy** library called **'argrelextrema'** that will allow us to extract the relative maximum points from our dataset. For more details about this algorithm, check out the document section: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.argrelextrema.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets import the algorithm\nfrom scipy.signal import argrelextrema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using this algorithm, we want to create a pandas dataframe to capture all the extreme points in our dataset that captures all the peak points per identity group. We will also want to do a scatter plot in the next section, and scatterplot implementation needs both values (x and y) to be numeric. So we also better create a column to represent the dates in a more numeric form. \n\nWe can do this by taking the date of the first comment as our reference point, and counting how many days the comment was made from the first comment in our dataset. Therefore, in the end we should get a dataframe with the following columns:\n\n| identity | created data | score | days_from_first |"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will store all the different datapoints in the following dataframe\npeaks_df = pd.DataFrame()\n\n# first we loop through all the different identities we are interested in\nfor col in races + religions + sexual_orientation:\n    # we pass the values through the algorithm to get an index of the rel. maximums \n    _max_index = argrelextrema(identity_weight_per_date_df[col].values, np.greater, order=15)\n    # we use the index returned to create a dataframe of the values for those index. in this case \n    # we are interested in the created date and the score, notice how the dataframe needs to be \n    # transformed because of the orientation of the arrays we started off with\n    col_peaks_df = pd.DataFrame(data = [identity_weight_per_date_df.index[_max_index], identity_weight_per_date_df[col].values[_max_index]]).T\n    col_peaks_df.columns = ['created_date','score']\n    # we create a new column labeling the identity so we can track which peak came from which identity\n    col_peaks_df['identity'] = col\n    # and we keep appending to our main dataframe \n    peaks_df = peaks_df.append(col_peaks_df)\n# lets set identity as our index and we are done\npeaks_df = peaks_df.set_index('identity')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to count the number of days from the first comment, we take our labeled data and convert the created date column\ncomments_with_date_df = train_df.loc[:, ['created_date', 'target','comment_text'] + identities].dropna()\ncomments_with_date_df['created_date'] = pd.to_datetime(with_date_df['created_date'].apply(lambda dt: dt[:10]))\ncomments_with_date_df['comment_count'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate days from first comment\nfirst_dt = min(comments_with_date_df['created_date'].values)\nlast_dt = max(comments_with_date_df['created_date'].values)\npeaks_df['days_from_first'] = (peaks_df['created_date'] - first_dt).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here is a peak at what our peaks_df looks like\npeaks_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Section 5: Visualizing Relative Maximums"},{"metadata":{},"cell_type":"markdown","source":"Scatter plot is typically used to show if there is a correlation between two variables. However, in this case, we can also use scatterplot to draw the relative maximums as descrete points. The nice thing about a scatter plot is that we can visualize a third variable as the size of the point. Let's take advantage of this and use the number of comments made as the size of the relative maximum points."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets create a function that returns the peaks dataframe for a given identity\n# we also want to get the number of toxic comments made against that identity in the dataframe\ndef get_identity_peaks_df(identity, peaks_df, comments_with_date_df):\n    # select subset and sort\n    identity_peaks_df = peaks_df[peaks_df.index==identity].sort_values(by='score', ascending=False)\n    # change the score type to float\n    identity_peaks_df['score'] = identity_peaks_df.score.astype(float)\n    # use created date as the index so we can join over in later step\n    identity_peaks_df = identity_peaks_df.set_index('created_date')\n    # calculate how many toxic comments were made targetting the given identity group\n    identity_comment_count_df = comments_with_date_df[comments_with_date_df[identity] > 0][['created_date','comment_count']].groupby('created_date').sum()\n    # do an inner join to also get the total number of comments made that day for the given identity\n    identity_peaks_df = identity_peaks_df.join(identity_comment_count_df)\n    return identity_peaks_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to make our scatter plot more presentable we will set the max and min of our y axis\ny_lim_min = peaks_df['score'].max() + peaks_df['score'].max() / 3 # adding a little bit head room on y axis\ny_lim_max = peaks_df['score'].min() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now lets write a function that draws the scatter plot for a given identity\ndef identity_scatter_plot(identity, identity_peaks_df, y_lim_min, y_lim_max):\n    x = identity_peaks_df['days_from_first'].values\n    y = identity_peaks_df['score'].values\n    size = identity_peaks_df['comment_count'].values\n    label = identity_peaks_df['comment_count'].index\n    plt.figure(figsize=(15,7))\n    plt.tick_params(axis='both', which='major', labelsize=16)\n    plt.scatter(x, y, s=size, label=label)\n    plt.ylim(y_lim_max, y_lim_min)\n    axis_font = {'fontname':'Arial', 'size':'14'}\n    plt.title('Relative Maximums - Targeted Against '+ identity.capitalize() +' Identity', fontsize=15)\n    plt.xlabel('Comment Date', fontsize=15)\n    plt.ylabel('Relative Weighted Toxic Score', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to plot each relative maximums per identity group. However, lets only look at top 5 for now. If you remember the horizontal bar plot we did at the start of our tutorial (second graph from the top), you can see that the identities that are most frequently targeted are white, black, homosexual_gay_or_lesbian, muslim, and jewish."},{"metadata":{"trusted":true},"cell_type":"code","source":"identity = 'white'\nidentity_peaks_df = get_identity_peaks_df(identity, peaks_df, comments_with_date_df)\nidentity_scatter_plot(identity, identity_peaks_df, y_lim_min, y_lim_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity = 'black'\nidentity_peaks_df = get_identity_peaks_df(identity, peaks_df, comments_with_date_df)\nidentity_scatter_plot(identity, identity_peaks_df, y_lim_min, y_lim_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity = 'homosexual_gay_or_lesbian'\nidentity_peaks_df = get_identity_peaks_df(identity, peaks_df, comments_with_date_df)\nidentity_scatter_plot(identity, identity_peaks_df, y_lim_min, y_lim_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity = 'muslim'\nidentity_peaks_df = get_identity_peaks_df(identity, peaks_df, comments_with_date_df)\nidentity_scatter_plot(identity, identity_peaks_df, y_lim_min, y_lim_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity = 'jewish'\nidentity_peaks_df = get_identity_peaks_df(identity, peaks_df, comments_with_date_df)\nidentity_scatter_plot(identity, identity_peaks_df, y_lim_min, y_lim_max)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is great! I will write down the analysis later! Meanwhile, if you have a great observation, write it down in the comment section =) "},{"metadata":{},"cell_type":"markdown","source":"## Section 6: Correlation and Heatmap of Identities"},{"metadata":{},"cell_type":"markdown","source":"To better understand the dataset, we need to look at the correlation between each identities. Thus, in this section we will calculate the correlation between each of the identities and see if identities are often mentioned together. We are also interested to know which identities are frequently mentioned (or not mentioned) together. Understanding correlation for a large number of columns would be very difficult without visualization. However, this task is very simple once you draw a heatmap.\n\nTo do this we will use Seaborn, a library that wraps around matplotlib and makes plotting easier to use. The full documentation for plotting a heatmap for correlation matrix can be found in the [documentation](https://seaborn.pydata.org/examples/many_pairwise_correlations.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets import seaborn\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = comments_with_date_df[identities].corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nsns.set(font_scale = 1)\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The heatmap plot of the correlation between the identities is very insightful. I will summarize my observations below. As always, if you see something interesting please mention it to me in the comment section.\n* It is interesting to see that strong correlations form triangular area at the edge of diagonal. \n* This basically means that there is a strong correlation between the groups of the identity (gender, religion, races, disabilities). This means, the comments where male identity is the target, female identity is also very likely to be the target.\n* In another words, in toxic and non-toxic comments, **people tend to make it about one group vs another quiet frequently**."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}