{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 라이브러리 로드"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport re\nimport operator \n\nimport numpy as np\nimport pandas as pd\n\nfrom gensim.models import KeyedVectors\n\nfrom sklearn import model_selection\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Input, Dense, CuDNNGRU,concatenate, Bidirectional, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\n\nimport seaborn as sns","execution_count":3,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\nUsing TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# 데이터 로드"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ntest = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)\n\n\ntrain.head()\n","execution_count":4,"outputs":[{"output_type":"stream","text":"Train shape :  (1804874, 45)\nTest shape :  (97320, 2)\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"      id            ...             toxicity_annotator_count\n0  59848            ...                                    4\n1  59849            ...                                    4\n2  59852            ...                                    4\n3  59855            ...                                    4\n4  59856            ...                                   47\n\n[5 rows x 45 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>0.000000</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:41.987077+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>0.000000</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:42.870083+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>0.000000</td>\n      <td>This is such an urgent design problem; kudos t...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:45.222647+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>0.000000</td>\n      <td>Is this something I'll be able to install on m...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:47.601894+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>0.893617</td>\n      <td>haha you guys are a bunch of losers.</td>\n      <td>0.021277</td>\n      <td>0.0</td>\n      <td>0.021277</td>\n      <td>0.87234</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015-09-29 10:50:48.488476+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"        id                                       comment_text\n0  7000000  Jeff Sessions is another one of Trump's Orwell...\n1  7000001  I actually inspected the infrastructure on Gra...\n2  7000002  No it won't . That's just wishful thinking on ...\n3  7000003  Instead of wringing our hands and nibbling the...\n4  7000004  how many of you commenters have garbage piled ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7000000</td>\n      <td>Jeff Sessions is another one of Trump's Orwell...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7000001</td>\n      <td>I actually inspected the infrastructure on Gra...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7000002</td>\n      <td>No it won't . That's just wishful thinking on ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7000003</td>\n      <td>Instead of wringing our hands and nibbling the...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7000004</td>\n      <td>how many of you commenters have garbage piled ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df에 train의 (id, comment_tex)와 test(id, comment_tex)를 결합해서 넣는다.\n#그리고 train, test는 지운다.\ndf = pd.concat([train[['id','comment_text']], test], axis=0) #pd.concat을 통해 데이터프레임을 결합한다.\ndel(train, test)\ngc.collect()\ndf.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"      id                                       comment_text\n0  59848  This is so cool. It's like, 'would you want yo...\n1  59849  Thank you!! This would make my life a lot less...\n2  59852  This is such an urgent design problem; kudos t...\n3  59855  Is this something I'll be able to install on m...\n4  59856               haha you guys are a bunch of losers.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>This is such an urgent design problem; kudos t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>Is this something I'll be able to install on m...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>haha you guys are a bunch of losers.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Embedding!\n\n#### Word Embedding은 Word를 R차원의 Vector로 매핑시켜주는 것을 말한다\n#### 비정형화된 Text를 숫자로 바꿔줌으로써 사람의 언어를 컴퓨터의 언어로 번역하는 것\n#### TF-IDF 등 여러 방법이 있지만\n#### FastText Common Crawl embeddings를 이용하자~"},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_common_crawl = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nembeddings_index = KeyedVectors.load_word2vec_format(ft_common_crawl)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect() #무엇?","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 텍스트 전처리하기!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#어휘를 생성하게 해주는 함수!\ndef build_vocab(texts): #texts를 받는다.\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#적용 범위 검사\ndef check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lower()를 이용하여 소문자로 만들기"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['comment_text'] = df['comment_text'].apply(lambda x: x.lower()) #소문자 만들기\ngc.collect()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['comment_text']) #어휘 생성\noov = check_coverage(vocab, embeddings_index)\noov[:10]","execution_count":12,"outputs":[{"output_type":"stream","text":"Found embeddings for 12.870% of vocab\nFound embeddings for  91.201% of all text\n","name":"stdout"},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"[(\"i'm\", 87786),\n (\"i've\", 28451),\n ('\"the', 26467),\n (\"trump's\", 25755),\n (\"let's\", 24190),\n (\"aren't\", 22808),\n (\"wouldn't\", 22241),\n ('so,', 21391),\n (\"wasn't\", 20224),\n (\"i'd\", 19329)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# FastText()의 수축 문제 (was not -> wasn't)을 고쳐보자!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#수축 문제를 고쳐서 mapping 시키자!\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(vocab,oov)\ngc.collect()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# FastText에서 다운 받은 '알려진' 수축 문제 확인"},{"metadata":{"trusted":true},"cell_type":"code","source":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"- Known Contractions -\")\nprint(\"   FastText :\")\nprint(known_contractions(embeddings_index))","execution_count":17,"outputs":[{"output_type":"stream","text":"- Known Contractions -\n   FastText :\n[\"can't\", \"'cause\", \"didn't\", \"doesn't\", \"don't\", \"haven't\", \"he's\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it's\", \"she's\", \"that's\", \"there's\", \"they're\", \"we're\", \"won't\", \"you'll\", \"you're\"]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 수축 문제를 해결하여 Data 정제!\n#### 우리 Data에 contraction mapping을 매핑시켜서 정제한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['comment_text'] = df['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['comment_text'].head()","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"0    this is so cool. it is like, 'would you want y...\n1    thank you!! this would make my life a lot less...\n2    this is such an urgent design problem; kudos t...\n3    is this something i will be able to install on...\n4                 haha you guys are a bunch of losers.\nName: comment_text, dtype: object"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# df['comment_text']의 vocab 구하고, oov확인!"},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)\noov[:10]","execution_count":22,"outputs":[{"output_type":"stream","text":"Found embeddings for 12.945% of vocab\nFound embeddings for  91.708% of all text\n","name":"stdout"},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"[(\"trump's\", 28503),\n ('\"the', 26467),\n ('so,', 21391),\n ('no,', 17856),\n ('trump.', 17578),\n ('it?', 12995),\n ('trump,', 11879),\n ('but,', 10540),\n ('fact,', 10043),\n ('\"i', 9931)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 구두점들 ', \" . ? 등'의 문제를 해결해야 한다!"},{"metadata":{"trusted":true},"cell_type":"code","source":"del(vocab,oov)\ngc.collect()","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 구두점들 cleaning!"},{"metadata":{"trusted":true},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(unknown_punct(embeddings_index, punct))","execution_count":26,"outputs":[{"output_type":"stream","text":"_ ` \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct_mapping = {\"_\":\" \", \"`\":\" \"}","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])    \n    for p in punct:\n        text = text.replace(p, f' {p} ')     \n    return text","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['comment_text'] = df['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)\n","execution_count":30,"outputs":[{"output_type":"stream","text":"Found embeddings for 47.128% of vocab\nFound embeddings for  99.520% of all text\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:100]","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"[('khadr', 6316),\n ('murkowski', 3601),\n ('siemian', 2178),\n ('sb21', 1986),\n ('mulroney', 1941),\n ('chretien', 1863),\n ('notley', 1708),\n ('theglobeandmail', 1423),\n ('manafort', 1417),\n ('djou', 1290),\n ('gorsuch', 1214),\n ('alceste', 1169),\n ('albertans', 1168),\n ('horgan', 1089),\n ('begich', 1057),\n ('sloter', 1034),\n ('usccb', 1015),\n ('gabbard', 1003),\n ('kealoha', 962),\n ('tfsa', 959),\n ('kpmg', 949),\n ('jpii', 898),\n ('trumpster', 890),\n ('guptas', 848),\n ('sb91', 841),\n ('conoco', 814),\n ('chaput', 786),\n ('klastri', 754),\n ('imua', 754),\n ('arpaio', 732),\n ('mcguinty', 701),\n ('fptp', 664),\n ('massengill', 662),\n ('shannyn', 660),\n ('punahou', 640),\n ('inouye', 635),\n ('raitt', 634),\n ('mulcair', 615),\n ('anwr', 601),\n ('ibbitson', 600),\n ('kapolei', 600),\n ('heco', 592),\n ('zinke', 580),\n ('poloz', 575),\n ('rhyner', 574),\n ('monsef', 570),\n ('coghill', 561),\n ('mufi', 559),\n ('sajjan', 548),\n ('ᴀɴᴅ', 540),\n ('wohlforth', 538),\n ('helfrich', 532),\n ('dprk', 528),\n ('hanabusa', 525),\n ('osweiler', 522),\n ('ontarians', 518),\n ('cmhc', 509),\n ('pfds', 504),\n ('auwe', 500),\n ('dlnr', 499),\n ('clallam', 494),\n ('hirono', 492),\n ('tokuda', 491),\n ('lw1', 484),\n ('nenshi', 481),\n ('cupich', 473),\n ('renzetti', 460),\n ('kakaako', 458),\n ('zille', 456),\n ('coupeville', 456),\n ('scaramucci', 447),\n ('hilcorp', 447),\n ('waianae', 442),\n ('chenault', 442),\n ('nationalpost', 438),\n ('giessel', 438),\n ('mililani', 423),\n ('hickenlooper', 420),\n ('prudhoe', 420),\n ('trumpsters', 416),\n ('trumpian', 410),\n ('cayetano', 402),\n ('southey', 400),\n ('goodale', 395),\n ('spenard', 395),\n ('splc', 394),\n ('wiliki', 389),\n ('micciche', 384),\n ('onkey', 382),\n ('2gtbpns', 381),\n ('ramaphosa', 374),\n ('shibai', 373),\n ('ᴛʜᴇ', 371),\n ('rrsps', 365),\n ('ahca', 365),\n ('denverpost', 363),\n ('hilliary', 359),\n ('m103', 358),\n ('saullie', 358),\n ('ʜᴏᴍᴇ', 358)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(vocab,oov)\ngc.collect()","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 욕(Swear_words)들을 대체하자!"},{"metadata":{},"cell_type":"markdown","source":"### 욕들 목록"},{"metadata":{"trusted":true},"cell_type":"code","source":"swear_words = [\n    ' 4r5e ',\n    ' 5h1t ',\n    ' 5hit ',\n    ' a55 ',\n    ' anal ',\n    ' anus ',\n    ' ar5e ',\n    ' arrse ',\n    ' arse ',\n    ' ass ',\n    ' ass-fucker ',\n    ' asses ',\n    ' assfucker ',\n    ' assfukka ',\n    ' asshole ',\n    ' assholes ',\n    ' asswhole ',\n    ' a_s_s ',\n    ' b!tch ',\n    ' b00bs ',\n    ' b17ch ',\n    ' b1tch ',\n    ' ballbag ',\n    ' balls ',\n    ' ballsack ',\n    ' bastard ',\n    ' beastial ',\n    ' beastiality ',\n    ' bellend ',\n    ' bestial ',\n    ' bestiality ',\n    ' biatch ',\n    ' bitch ',\n    ' bitcher ',\n    ' bitchers ',\n    ' bitches ',\n    ' bitchin ',\n    ' bitching ',\n    ' bloody ',\n    ' blow job ',\n    ' blowjob ',\n    ' blowjobs ',\n    ' boiolas ',\n    ' bollock ',\n    ' bollok ',\n    ' boner ',\n    ' boob ',\n    ' boobs ',\n    ' booobs ',\n    ' boooobs ',\n    ' booooobs ',\n    ' booooooobs ',\n    ' breasts ',\n    ' buceta ',\n    ' bugger ',\n    ' bum ',\n    ' bunny fucker ',\n    ' butt ',\n    ' butthole ',\n    ' buttmuch ',\n    ' buttplug ',\n    ' c0ck ',\n    ' c0cksucker ',\n    ' carpet muncher ',\n    ' cawk ',\n    ' chink ',\n    ' cipa ',\n    ' cl1t ',\n    ' clit ',\n    ' clitoris ',\n    ' clits ',\n    ' cnut ',\n    ' cock ',\n    ' cock-sucker ',\n    ' cockface ',\n    ' cockhead ',\n    ' cockmunch ',\n    ' cockmuncher ',\n    ' cocks ',\n    ' cocksuck ',\n    ' cocksucked ',\n    ' cocksucker ',\n    ' cocksucking ',\n    ' cocksucks ',\n    ' cocksuka ',\n    ' cocksukka ',\n    ' cok ',\n    ' cokmuncher ',\n    ' coksucka ',\n    ' coon ',\n    ' cox ',\n    ' crap ',\n    ' cum ',\n    ' cummer ',\n    ' cumming ',\n    ' cums ',\n    ' cumshot ',\n    ' cunilingus ',\n    ' cunillingus ',\n    ' cunnilingus ',\n    ' cunt ',\n    ' cuntlick ',\n    ' cuntlicker ',\n    ' cuntlicking ',\n    ' cunts ',\n    ' cyalis ',\n    ' cyberfuc ',\n    ' cyberfuck ',\n    ' cyberfucked ',\n    ' cyberfucker ',\n    ' cyberfuckers ',\n    ' cyberfucking ',\n    ' d1ck ',\n    ' damn ',\n    ' dick ',\n    ' dickhead ',\n    ' dildo ',\n    ' dildos ',\n    ' dink ',\n    ' dinks ',\n    ' dirsa ',\n    ' dlck ',\n    ' dog-fucker ',\n    ' doggin ',\n    ' dogging ',\n    ' donkeyribber ',\n    ' doosh ',\n    ' duche ',\n    ' dyke ',\n    ' ejaculate ',\n    ' ejaculated ',\n    ' ejaculates ',\n    ' ejaculating ',\n    ' ejaculatings ',\n    ' ejaculation ',\n    ' ejakulate ',\n    ' f u c k ',\n    ' f u c k e r ',\n    ' f4nny ',\n    ' fag ',\n    ' fagging ',\n    ' faggitt ',\n    ' faggot ',\n    ' faggs ',\n    ' fagot ',\n    ' fagots ',\n    ' fags ',\n    ' fanny ',\n    ' fannyflaps ',\n    ' fannyfucker ',\n    ' fanyy ',\n    ' fatass ',\n    ' fcuk ',\n    ' fcuker ',\n    ' fcuking ',\n    ' feck ',\n    ' fecker ',\n    ' felching ',\n    ' fellate ',\n    ' fellatio ',\n    ' fingerfuck ',\n    ' fingerfucked ',\n    ' fingerfucker ',\n    ' fingerfuckers ',\n    ' fingerfucking ',\n    ' fingerfucks ',\n    ' fistfuck ',\n    ' fistfucked ',\n    ' fistfucker ',\n    ' fistfuckers ',\n    ' fistfucking ',\n    ' fistfuckings ',\n    ' fistfucks ',\n    ' flange ',\n    ' fook ',\n    ' fooker ',\n    ' fuck ',\n    ' fucka ',\n    ' fucked ',\n    ' fucker ',\n    ' fuckers ',\n    ' fuckhead ',\n    ' fuckheads ',\n    ' fuckin ',\n    ' fucking ',\n    ' fuckings ',\n    ' fuckingshitmotherfucker ',\n    ' fuckme ',\n    ' fucks ',\n    ' fuckwhit ',\n    ' fuckwit ',\n    ' fudge packer ',\n    ' fudgepacker ',\n    ' fuk ',\n    ' fuker ',\n    ' fukker ',\n    ' fukkin ',\n    ' fuks ',\n    ' fukwhit ',\n    ' fukwit ',\n    ' fux ',\n    ' fux0r ',\n    ' f_u_c_k ',\n    ' gangbang ',\n    ' gangbanged ',\n    ' gangbangs ',\n    ' gaylord ',\n    ' gaysex ',\n    ' goatse ',\n    ' God ',\n    ' god-dam ',\n    ' god-damned ',\n    ' goddamn ',\n    ' goddamned ',\n    ' hardcoresex ',\n    ' hell ',\n    ' heshe ',\n    ' hoar ',\n    ' hoare ',\n    ' hoer ',\n    ' homo ',\n    ' hore ',\n    ' horniest ',\n    ' horny ',\n    ' hotsex ',\n    ' jack-off ',\n    ' jackoff ',\n    ' jap ',\n    ' jerk-off ',\n    ' jism ',\n    ' jiz ',\n    ' jizm ',\n    ' jizz ',\n    ' kawk ',\n    ' knob ',\n    ' knobead ',\n    ' knobed ',\n    ' knobend ',\n    ' knobhead ',\n    ' knobjocky ',\n    ' knobjokey ',\n    ' kock ',\n    ' kondum ',\n    ' kondums ',\n    ' kum ',\n    ' kummer ',\n    ' kumming ',\n    ' kums ',\n    ' kunilingus ',\n    ' l3itch ',\n    ' labia ',\n    ' lmfao ',\n    ' lust ',\n    ' lusting ',\n    ' m0f0 ',\n    ' m0fo ',\n    ' m45terbate ',\n    ' ma5terb8 ',\n    ' ma5terbate ',\n    ' masochist ',\n    ' master-bate ',\n    ' masterb8 ',\n    ' masterbat3 ',\n    ' masterbate ',\n    ' masterbation ',\n    ' masterbations ',\n    ' masturbate ',\n    ' mo-fo ',\n    ' mof0 ',\n    ' mofo ',\n    ' mothafuck ',\n    ' mothafucka ',\n    ' mothafuckas ',\n    ' mothafuckaz ',\n    ' mothafucked ',\n    ' mothafucker ',\n    ' mothafuckers ',\n    ' mothafuckin ',\n    ' mothafucking ',\n    ' mothafuckings ',\n    ' mothafucks ',\n    ' mother fucker ',\n    ' motherfuck ',\n    ' motherfucked ',\n    ' motherfucker ',\n    ' motherfuckers ',\n    ' motherfuckin ',\n    ' motherfucking ',\n    ' motherfuckings ',\n    ' motherfuckka ',\n    ' motherfucks ',\n    ' muff ',\n    ' mutha ',\n    ' muthafecker ',\n    ' muthafuckker ',\n    ' muther ',\n    ' mutherfucker ',\n    ' n1gga ',\n    ' n1gger ',\n    ' nazi ',\n    ' nigg3r ',\n    ' nigg4h ',\n    ' nigga ',\n    ' niggah ',\n    ' niggas ',\n    ' niggaz ',\n    ' nigger ',\n    ' niggers ',\n    ' nob ',\n    ' nob jokey ',\n    ' nobhead ',\n    ' nobjocky ',\n    ' nobjokey ',\n    ' numbnuts ',\n    ' nutsack ',\n    ' orgasim ',\n    ' orgasims ',\n    ' orgasm ',\n    ' orgasms ',\n    ' p0rn ',\n    ' pawn ',\n    ' pecker ',\n    ' penis ',\n    ' penisfucker ',\n    ' phonesex ',\n    ' phuck ',\n    ' phuk ',\n    ' phuked ',\n    ' phuking ',\n    ' phukked ',\n    ' phukking ',\n    ' phuks ',\n    ' phuq ',\n    ' pigfucker ',\n    ' pimpis ',\n    ' piss ',\n    ' pissed ',\n    ' pisser ',\n    ' pissers ',\n    ' pisses ',\n    ' pissflaps ',\n    ' pissin ',\n    ' pissing ',\n    ' pissoff ',\n    ' poop ',\n    ' porn ',\n    ' porno ',\n    ' pornography ',\n    ' pornos ',\n    ' prick ',\n    ' pricks ',\n    ' pron ',\n    ' pube ',\n    ' pusse ',\n    ' pussi ',\n    ' pussies ',\n    ' pussy ',\n    ' pussys ',\n    ' rectum ',\n    ' retard ',\n    ' rimjaw ',\n    ' rimming ',\n    ' s hit ',\n    ' s.o.b. ',\n    ' sadist ',\n    ' schlong ',\n    ' screwing ',\n    ' scroat ',\n    ' scrote ',\n    ' scrotum ',\n    ' semen ',\n    ' sex ',\n    ' sh!t ',\n    ' sh1t ',\n    ' shag ',\n    ' shagger ',\n    ' shaggin ',\n    ' shagging ',\n    ' shemale ',\n    ' shit ',\n    ' shitdick ',\n    ' shite ',\n    ' shited ',\n    ' shitey ',\n    ' shitfuck ',\n    ' shitfull ',\n    ' shithead ',\n    ' shiting ',\n    ' shitings ',\n    ' shits ',\n    ' shitted ',\n    ' shitter ',\n    ' shitters ',\n    ' shitting ',\n    ' shittings ',\n    ' shitty ',\n    ' skank ',\n    ' slut ',\n    ' sluts ',\n    ' smegma ',\n    ' smut ',\n    ' snatch ',\n    ' son-of-a-bitch ',\n    ' spac ',\n    ' spunk ',\n    ' s_h_i_t ',\n    ' t1tt1e5 ',\n    ' t1tties ',\n    ' teets ',\n    ' teez ',\n    ' testical ',\n    ' testicle ',\n    ' tit ',\n    ' titfuck ',\n    ' tits ',\n    ' titt ',\n    ' tittie5 ',\n    ' tittiefucker ',\n    ' titties ',\n    ' tittyfuck ',\n    ' tittywank ',\n    ' titwank ',\n    ' tosser ',\n    ' turd ',\n    ' tw4t ',\n    ' twat ',\n    ' twathead ',\n    ' twatty ',\n    ' twunt ',\n    ' twunter ',\n    ' v14gra ',\n    ' v1gra ',\n    ' vagina ',\n    ' viagra ',\n    ' vulva ',\n    ' w00se ',\n    ' wang ',\n    ' wank ',\n    ' wanker ',\n    ' wanky ',\n    ' whoar ',\n    ' whore ',\n    ' willies ',\n    ' willy ',\n    ' xrated ',\n    ' xxx '    \n]","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Swears에는 있고 embeddings_index에는 없는 단어들을 replace_with_fuck에 집어 넣기!"},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_with_fuck = []\n\nfor swear in swear_words:\n    if swear[1:(len(swear)-1)] not in embeddings_index:\n        replace_with_fuck.append(swear)\n        \nreplace_with_fuck = '|'.join(replace_with_fuck)\nreplace_with_fuck\n        ","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"' 4r5e | 5h1t | 5hit | ass-fucker | assfucker | assfukka | asswhole | a_s_s | b!tch | b17ch | blow job | boiolas | bollok | boooobs | booooobs | booooooobs | bunny fucker | buttmuch | c0cksucker | carpet muncher | cl1t | cockface | cockmunch | cockmuncher | cocksuka | cocksukka | cokmuncher | coksucka | cunillingus | cuntlick | cuntlicker | cuntlicking | cyalis | cyberfuc | cyberfuck | cyberfucked | cyberfucker | cyberfuckers | cyberfucking | dirsa | dlck | dog-fucker | donkeyribber | ejaculatings | ejakulate | f u c k | f u c k e r | f4nny | faggitt | faggs | fannyflaps | fannyfucker | fanyy | fingerfucker | fingerfuckers | fingerfucks | fistfuck | fistfucked | fistfucker | fistfuckers | fistfucking | fistfuckings | fistfucks | fuckingshitmotherfucker | fuckwhit | fudge packer | fudgepacker | fukwhit | fukwit | fux0r | f_u_c_k | god-dam | kawk | knobead | knobed | knobend | knobjocky | knobjokey | kondum | kondums | kummer | kumming | kums | kunilingus | l3itch | m0f0 | m0fo | m45terbate | ma5terb8 | ma5terbate | master-bate | masterb8 | masterbat3 | masterbations | mof0 | mothafuck | mothafuckaz | mothafucked | mothafucking | mothafuckings | mothafucks | mother fucker | motherfucked | motherfuckings | motherfuckka | motherfucks | muthafecker | muthafuckker | n1gga | n1gger | nigg3r | nigg4h | nob jokey | nobjocky | nobjokey | penisfucker | phuked | phuking | phukked | phukking | phuks | phuq | pigfucker | pimpis | pissflaps | rimjaw | s hit | scroat | sh!t | shitdick | shitfull | shitings | shittings | s_h_i_t | t1tt1e5 | t1tties | teez | tittie5 | tittiefucker | tittywank | tw4t | twathead | twunter | v14gra | v1gra | w00se | whoar '"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def handle_swears(text):\n    text = re.sub(replace_with_fuck, ' fuck ', text) #text에서 replace_with_fuck과 일치되는 것들을 다 fuck으로 대체한다.\n    return text","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# comment_text에서 replace_with_fuck의 단어들을 fuck으로 처리한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['comment_text'] = df['comment_text'].apply(lambda x: handle_swears(x))\ngc.collect()","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Train / Test 데이터 준비!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.iloc[:1804874,:]\ntest = df.iloc[1804874:,:]\n\ntrain.head()","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"      id                                       comment_text\n0  59848  this is so cool .  it is like ,    '  would yo...\n1  59849  thank you !  !  this would make my life a lot ...\n2  59852  this is such an urgent design problem ;  kudos...\n3  59855  is this something i will be able to install on...\n4  59856             haha you guys are a bunch of losers . ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>this is so cool .  it is like ,    '  would yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>thank you !  !  this would make my life a lot ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>this is such an urgent design problem ;  kudos...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>is this something i will be able to install on...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>haha you guys are a bunch of losers .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(df)\ngc.collect()","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"19"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 데이터 추가 작업"},{"metadata":{},"cell_type":"markdown","source":"#### 원본 데이터"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_orig = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\ntrain_orig.head()","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"      id            ...             toxicity_annotator_count\n0  59848            ...                                    4\n1  59849            ...                                    4\n2  59852            ...                                    4\n3  59855            ...                                    4\n4  59856            ...                                   47\n\n[5 rows x 45 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>0.000000</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:41.987077+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>0.000000</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:42.870083+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>0.000000</td>\n      <td>This is such an urgent design problem; kudos t...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:45.222647+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>0.000000</td>\n      <td>Is this something I'll be able to install on m...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:47.601894+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>0.893617</td>\n      <td>haha you guys are a bunch of losers.</td>\n      <td>0.021277</td>\n      <td>0.0</td>\n      <td>0.021277</td>\n      <td>0.87234</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015-09-29 10:50:48.488476+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### 정제한 train셋에 원래 train 셋에 있었던 target을 붙여서 train 셋을 만든다."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train,train_orig[['target']]],axis=1)\ntrain.head()","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"      id                                       comment_text    target\n0  59848  this is so cool .  it is like ,    '  would yo...  0.000000\n1  59849  thank you !  !  this would make my life a lot ...  0.000000\n2  59852  this is such an urgent design problem ;  kudos...  0.000000\n3  59855  is this something i will be able to install on...  0.000000\n4  59856             haha you guys are a bunch of losers .   0.893617","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>this is so cool .  it is like ,    '  would yo...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>thank you !  !  this would make my life a lot ...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>this is such an urgent design problem ;  kudos...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>is this something i will be able to install on...</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>haha you guys are a bunch of losers .</td>\n      <td>0.893617</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(train_orig)\ngc.collect()","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"32"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### target을 binary로 변형하기!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'] = np.where(train['target'] >= 0.5, True, False) #target이 0.5이상이면 True로 변환, 그렇지 않으면 False","execution_count":42,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### train 셋을 train / validation split하기"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, validate_df = model_selection.train_test_split(train, test_size=0.1)\nprint('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))","execution_count":43,"outputs":[{"output_type":"stream","text":"1624386 train comments, 180488 validate comments\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 텍스트 Tokenize로 토큰화 하기!"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NUM_WORDS = 100000\nTOXICITY_COLUMN = 'target'\nTEXT_COLUMN = 'comment_text'\n\n# Tokenizer 생성 후 train_df 토큰화 하기!\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(train_df[TEXT_COLUMN])\n\nMAX_SEQUENCE_LENGTH = 256\ndef pad_text(texts, tokenizer):\n    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)","execution_count":44,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embedding matrix 만들기"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDINGS_DIMENSION = 300\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1,EMBEDDINGS_DIMENSION))","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words_in_embedding = 0\n\nfor word, i in tokenizer.word_index.items():\n    if word in embeddings_index.vocab:\n        embedding_vector = embeddings_index[word]\n        embedding_matrix[i] = embedding_vector        \n        num_words_in_embedding += 1","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\ntrain_labels = train_df[TOXICITY_COLUMN]\nvalidate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\nvalidate_labels = validate_df[TOXICITY_COLUMN]","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 모델 구성!"},{"metadata":{},"cell_type":"markdown","source":"#### GRU는 LSTM의 장점을 유지하면서도 계산복잡성을 확 낮춘 셀 구조입니다.\n#### GRU도 Gradient Vanishing/Explosion 문제를 극복했다는 점에서 LSTM과 유사하지만 게이트 일부를 생략한 형태입니다. "},{"metadata":{},"cell_type":"markdown","source":"#### cuDNN은 엔비디아 CUDA 딥 뉴럴 네트워크 라이브러리, 즉 딥 뉴럴 네트워크를 위한 GPU 가속화 라이브러리의 기초 요소로 컨볼루션(Convolution), 풀링(Pooling), 표준화(Nomarlization), 활성화(Activation)와 같은 일반적인 루틴을 빠르게 이행할 수 있도록 하는 라이브러리입니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델의 층을 구성하자!\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\n# Embedding(단어 집합의 크기, 출력 차원, 입력 시퀀스의 길이)\nembedding_layer = Embedding(len(tokenizer.word_index) + 1,\n                            EMBEDDINGS_DIMENSION,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\nx = embedding_layer(sequence_input)\n\n# Dropout :  과적합을 방지하기 위해서 학습 시에 지정된 비율만큼 임의의 입력 뉴런(1차원)을 제외시킵니다.\nx = SpatialDropout1D(0.2)(x)\n\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)   \n\n#필터를 이용하여 지역적인 특징을 추출합니다.\nx = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n\navg_pool1 = GlobalAveragePooling1D()(x)\nmax_pool1 = GlobalMaxPooling1D()(x) #입력벡터에서 특정 구간마다 값을 골라 벡터를 구성한 후 반환합니다.\n\n#avg_pool과 max_pool 배열 연결\nx = concatenate([avg_pool1, max_pool1])\n\n# Dense는 모든 입력 뉴런과 출력 뉴런을 연결하는 전결합층입니다. 결과물을 sigmoid를 통해 분류한다.\n# 시그모이드 함수의 사용 이유는 비선형적으로 출력값을 매끄럽게 하기 위한 것\npreds = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.summary()","execution_count":50,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 256)          0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 256, 300)     90315000    input_1[0][0]                    \n__________________________________________________________________________________________________\nspatial_dropout1d_1 (SpatialDro (None, 256, 300)     0           embedding_1[0][0]                \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 256, 128)     140544      spatial_dropout1d_1[0][0]        \n__________________________________________________________________________________________________\nconv1d_1 (Conv1D)               (None, 255, 64)      16448       bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nglobal_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n__________________________________________________________________________________________________\nglobal_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n                                                                 global_max_pooling1d_1[0][0]     \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n==================================================================================================\nTotal params: 90,472,121\nTrainable params: 157,121\nNon-trainable params: 90,315,000\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer=Adam(),\n              metrics=['acc'])","execution_count":51,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 1024\nNUM_EPOCHS = 100","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(\n    train_text,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n    validation_data=(validate_text, validate_labels),\n    callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)])\n","execution_count":null,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nTrain on 1624386 samples, validate on 180488 samples\nEpoch 1/100\n 398336/1624386 [======>.......................] - ETA: 2:20 - loss: 0.1888 - acc: 0.9309","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict & Submit"},{"metadata":{},"cell_type":"markdown","source":"Let's submit this as our first submission. Once we have a reasonable pipeline setup, we can move on to looking at the competition metric in more detail."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsubmission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}